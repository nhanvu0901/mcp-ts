model_list:
    # Azure OpenAI Chat Models - USING CORRECT LiteLLM ENV SYNTAX
    - model_name: ace-gpt-4o
      litellm_params:
          model: azure/ace-gpt-4o
          api_base: https://aceai-dev-swedencentral.openai.azure.com
          api_key: "os.environ/AZURE_OPENAI_API_KEY"
          api_version: 2025-01-01-preview
          rpm: 1000 # requests per minute limit
      model_info:
          mode: chat
          input_cost_per_token: 2.75e-06
          output_cost_per_token: 11.0e-06
          max_tokens: 128000

    - model_name: gpt-4o-mini
      litellm_params:
          model: azure/gpt-4o-mini
          api_base: https://aceai-dev-swedencentral.openai.azure.com
          api_key: "os.environ/AZURE_OPENAI_API_KEY"
          api_version: 2024-02-15-preview
          rpm: 1000 # requests per minute limit
      model_info:
          mode: chat
          input_cost_per_token: 0.165e-06
          output_cost_per_token: 0.66e-06
          max_tokens: 128000

    - model_name: o3-mini
      litellm_params:
          model: azure/o3-mini
          api_base: https://aceai-dev-swedencentral.openai.azure.com
          api_key: "os.environ/AZURE_OPENAI_API_KEY"
          api_version: 2024-12-01-preview
          rpm: 1000 # requests per minute limit
      model_info:
          mode: chat
          input_cost_per_token: 1.21e-06
          output_cost_per_token: 4.84e-06
          max_tokens: 128000

    - model_name: gpt-4.1
      litellm_params:
          model: azure/gpt-4.1
          api_base: https://aceai-dev-swedencentral.openai.azure.com
          api_key: "os.environ/AZURE_OPENAI_API_KEY"
          api_version: 2025-01-01-preview
          rpm: 1000 # requests per minute limit
      model_info:
          mode: chat
          input_cost_per_token: 2.0e-06
          output_cost_per_token: 8.0e-06
          max_tokens: 128000

    - model_name: o3
      litellm_params:
          model: azure/o3
          api_base: https://aceai-dev-swedencentral.openai.azure.com
          api_key: "os.environ/AZURE_OPENAI_API_KEY"
          api_version: 2025-01-01-preview
          rpm: 1000 # requests per minute limit
      model_info:
          mode: chat
          input_cost_per_token: 2.0e-06
          output_cost_per_token: 8.0e-06
          max_tokens: 128000

    # Azure OpenAI Embedding Model - USING CORRECT LiteLLM ENV SYNTAX
    - model_name: ace-text-embedding-3-large
      litellm_params:
          model: azure/ace-text-embedding-3-large
          api_base: https://aceai-dev-swedencentral.openai.azure.com
          api_key: "os.environ/AZURE_OPENAI_EMBEDDING_API_KEY"
          api_version: "os.environ/AZURE_OPENAI_EMBEDDING_MODEL_API_VERSION"
          rpm: 1000 # requests per minute limit
      model_info:
          mode: embedding
          input_cost_per_token: 13e-06
          max_input_tokens: 8191

# LiteLLM module level settings
litellm_settings:
    drop_params: true
    set_verbose: true
    # Enable request/response logging for debugging
    log_raw_request_response: true
    # Optional: Add success callbacks for tracking
    # success_callback: ["langfuse"]  # Uncomment if you want to use Langfuse

# General settings with database support
general_settings:
    # Database configuration
    database_url: "postgresql://litellm_user:litellm_password@postgres:5432/litellm"
    store_model_in_db: true

    # Authentication - USING CORRECT LiteLLM ENV SYNTAX
    master_key: "os.environ/LITELLM_MASTER_KEY"

    # Salt key for encryption
    salt_key: "os.environ/LITELLM_SALT_KEY"

    # Disable model fallbacks to avoid complexity
    enable_fallbacks: false

    # Request timeout (seconds)
    request_timeout: 600

    # Enable detailed logging
    set_verbose: true

    # Budget and usage controls
    max_budget: 1000 # Maximum spend limit in USD
    budget_duration: "30d" # Budget reset period

    # Rate limiting
    tpm_limit: 10000 # Tokens per minute limit
    rpm_limit: 1000 # Requests per minute limit

    # Disable caching initially to reduce complexity
    cache: false

    # Default user settings
    default_user_max_budget: 100
    default_user_budget_duration: "30d"

    # Optional: Enable alerting
    # alerting: ["slack"]  # Uncomment and set SLACK_WEBHOOK_URL if you want alerts

# Router settings (simplified)
router_settings:
    routing_strategy: "simple-shuffle"

    # Basic retry configuration
    num_retries: 3
    timeout: 60

    # Disable retry on specific error types to avoid the API version parsing issue
    retry_policy:
        BadRequestErrorRetries: 0
        AuthenticationErrorRetries: 0

# Pre-configured API keys for applications - USING CORRECT LiteLLM ENV SYNTAX
api_keys:
    - key: "os.environ/LITELLM_APP_KEY" # Static key for your application
      permissions:
          - "chat_completions"
          - "embeddings"
          - "models"
      models:
          - "gpt-4o"
          - "ace-text-embedding-3-large"
      user_id: "mcp-rag-app"
      team_id: "default"
      max_budget: 500
      budget_duration: "30d"
      rpm_limit: 100 # requests per minute for this key
      tpm_limit: 5000 # tokens per minute for this key

# UI Dashboard settings
ui_settings:
    page_title: "MCP RAG LiteLLM Proxy"
    description: "LiteLLM proxy for MCP RAG application"

    # Enable admin panel features
    enable_admin_ui: true
    enable_user_ui: true

# Security settings (basic)
security:
    # Enable CORS for web applications
    enable_cors: true
    allowed_origins: ["http://localhost:3000", "http://localhost:8080"]

    # API key validation
    strict_key_validation: false # Keep false for easier debugging
